{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from mmcv import Config\n",
    "from mmdet.apis import set_random_seed\n",
    "from mmdet.apis import init_detector, inference_detector\n",
    "from mmdet.datasets import build_dataset\n",
    "from mmdet.models import build_detector\n",
    "\n",
    "from xml_to_np import xml_to_np, all_xml_to_np\n",
    "from AP import get_single_image_results\n",
    "from soft_nms import py_cpu_softnms\n",
    "from dbscan_result_to_aligned_result import dbscan_result_to_aligned_result\n",
    "from utils import detect_quads, get_free_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=5.72s)\n",
      "creating index...\n",
      "index created!\n",
      "load checkpoint from local path: /home/aiarhipov/centernet/exps/16_paper_params_dla34_batch8/epoch_150.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-19 17:19:51,795 - root - INFO - ModulatedDeformConvPack neck.deconv_layers.0.conv is upgraded to version 2.\n",
      "2023-06-19 17:19:51,798 - root - INFO - ModulatedDeformConvPack neck.deconv_layers.2.conv is upgraded to version 2.\n",
      "2023-06-19 17:19:51,800 - root - INFO - ModulatedDeformConvPack neck.deconv_layers.4.conv is upgraded to version 2.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_file = '/home/aiarhipov/centernet/exps/16_paper_params_dla34_batch8/epoch_150.pth'\n",
    "\n",
    "config_file = \"/home/aiarhipov/centernet/exps/16_paper_params_dla34_batch8/config.py\"\n",
    "cfg = Config.fromfile(config_file)\n",
    "\n",
    "set_random_seed(0, deterministic=False)\n",
    "\n",
    "# Build dataset\n",
    "dataset = build_dataset(cfg.data.test)\n",
    "\n",
    "# Build the detector\n",
    "model = build_detector(cfg.model)\n",
    "# Add an attribute for visualization convenience\n",
    "model.CLASSES = dataset.CLASSES\n",
    "\n",
    "model = init_detector(config_file, checkpoint_file, device='cuda:1')\n",
    "# img_path = '/home/aiarhipov/centernet/imgs/0d995a743dccbda88af4955cd07ab5bf3b8b663e.jpg'  # or img = mmcv.imread(img), which will only load it once\n",
    "# res = inference_detector(model, img_path)\n",
    "# res1 = res\n",
    "# threshold = 0.5\n",
    "# show_result_pyplot(model, img_path, res, score_thr = threshold, out_file = \"/home/aiarhipov/centernet/imgs/output.jpg\")\n",
    "# res_algo2 = algo2_result_to_aligned_result(res1, threshold=threshold, r=0.2)\n",
    "# res_dbscan = dbscan_result_to_aligned_result(res1, threshold=threshold)\n",
    "# _res_algo2 = res_algo2\n",
    "# _res_dbscan = res_dbscan\n",
    "# show_result_pyplot(model, img_path, _res_algo2, score_thr = threshold, out_file = \"/home/aiarhipov/centernet/imgs/output_algo2.jpg\")\n",
    "# show_result_pyplot(model, img_path, _res_dbscan, score_thr = threshold, out_file = \"/home/aiarhipov/centernet/imgs/output_dbscan.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf_threshold = 0.5\n",
    "# iou_threshold = 0.9\n",
    "# for idx in range(int(len(dataset) / 100)):\n",
    "#     anno = dataset.get_ann_info(idx=idx)\n",
    "#     boxes = anno[\"bboxes\"]\n",
    "#     segm_path = anno[\"seg_map\"]\n",
    "#     xml_path = f\"/home/aiarhipov/datasets/WTW-dataset/test/xml/{segm_path[:-4]}.xml\"\n",
    "#     img_path = f\"/home/aiarhipov/datasets/WTW-dataset/test/images/{segm_path[:-4]}.jpg\"\n",
    "    \n",
    "#     gt_boxes = xml_to_np(xml_path)\n",
    "\n",
    "#     pred = inference_detector(model, img_path)[0]\n",
    "    \n",
    "#     conf_pred = pred[pred[:, 4] > conf_threshold]\n",
    "#     print(\"conf = \", conf_pred.shape[0])\n",
    "#     print(\"gt = \", gt_boxes.shape[0])\n",
    "    \n",
    "#     map_pred_to_gt_idx = []\n",
    "#     map_pred_to_gt_iou = []\n",
    "#     for idx_pred, pred_box in enumerate(conf_pred):\n",
    "#         best_match_idx = None\n",
    "#         best_match_iou = 0\n",
    "#         for idx_gt, gt_box in enumerate(gt_boxes):\n",
    "#             iou = calc_iou_individual(pred_box[:4].tolist(), gt_box[:4].tolist())\n",
    "#             if iou > best_match_iou and iou > iou_threshold:\n",
    "#                 best_match_iou = iou\n",
    "#                 best_match_idx = idx_gt\n",
    "#         map_pred_to_gt_idx.append(best_match_idx)\n",
    "#         map_pred_to_gt_iou.append(best_match_iou)\n",
    "    \n",
    "#     tp = len([x for x in map_pred_to_gt_idx if str(x) != \"nan\"])\n",
    "#     print(\"len map_pred_to_gt_idx or tp \", tp)\n",
    "#     print(\"map_pred_to_gt_iou \", map_pred_to_gt_iou)\n",
    "#     print(f\"precision = {tp / conf_pred.shape[0]}\")\n",
    "#     print(f\"recall = {tp / gt_boxes.shape[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physical structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf_threshold = 0.5\n",
    "# iou_threshold = 0.9\n",
    "# res = []\n",
    "# for idx in range(int(len(dataset))):\n",
    "#     anno = dataset.get_ann_info(idx=idx)\n",
    "#     boxes = anno[\"bboxes\"]\n",
    "#     segm_path = anno[\"seg_map\"]\n",
    "#     xml_path = f\"/home/aiarhipov/datasets/WTW-dataset/test/xml/{segm_path[:-4]}.xml\"\n",
    "#     img_path = f\"/home/aiarhipov/datasets/WTW-dataset/test/images/{segm_path[:-4]}.jpg\"\n",
    "    \n",
    "#     gt_boxes = xml_to_np(xml_path)\n",
    "    \n",
    "#     pred = inference_detector(model, img_path)[0]\n",
    "    \n",
    "#     conf_indexes = py_cpu_softnms(pred[:, :4], pred[:, 4], thresh=0.475, method=2)\n",
    "#     # print(pred[conf_indexes])\n",
    "#     conf_pred = pred[conf_indexes]\n",
    "#     # conf_pred = pred[pred[:, 4] > conf_threshold]\n",
    "#     # conf_pred = pred[:int(gt_boxes.shape[0] * 1.5)]\n",
    "#     metrics = get_single_image_results(gt_boxes[:, :4].tolist(), conf_pred[:, :4].tolist(), iou_thr=iou_threshold)\n",
    "    \n",
    "#     if (metrics[\"true_pos\"] + metrics[\"false_pos\"]) != 0:\n",
    "#         metrics[\"Precision\"] = metrics[\"true_pos\"] / (metrics[\"true_pos\"] + metrics[\"false_pos\"])\n",
    "#     else:\n",
    "#         metrics[\"Precision\"] = 0\n",
    "        \n",
    "#     if (metrics[\"true_pos\"] + metrics[\"false_neg\"]) != 0:\n",
    "#         metrics[\"Recall\"] = metrics[\"true_pos\"] / (metrics[\"true_pos\"] + metrics[\"false_neg\"])\n",
    "#     else:\n",
    "#         metrics[\"Recall\"]\n",
    "        \n",
    "#     if (metrics[\"Precision\"] + metrics[\"Recall\"]) != 0:\n",
    "#         metrics[\"F1\"] = 2 * metrics[\"Precision\"] * metrics[\"Recall\"] / (metrics[\"Precision\"] + metrics[\"Recall\"])\n",
    "#     else:\n",
    "#         metrics[\"F1\"] = 0\n",
    "        \n",
    "#     res.append(metrics)\n",
    "    \n",
    "# Precision = np.mean([d[\"Precision\"] for d in res])\n",
    "# Recall = np.mean([d[\"Recall\"] for d in res])\n",
    "# F1 = np.mean([d[\"F1\"] for d in res])\n",
    "\n",
    "# print(f\"Precision = {Precision}\")\n",
    "# print(f\"Recall = {Recall}\")\n",
    "# print(f\"F1 = {F1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dbscan before Soft-nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7147a37c4341494a91b22e15e4801f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3611 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m gt_boxes \u001b[39m=\u001b[39m xml_to_np(xml_path)\n\u001b[1;32m     13\u001b[0m pred \u001b[39m=\u001b[39m inference_detector(model, img_path)[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 14\u001b[0m conf_indexes \u001b[39m=\u001b[39m py_cpu_softnms(pred[:, :\u001b[39m4\u001b[39;49m], pred[:, \u001b[39m4\u001b[39m], thresh\u001b[39m=\u001b[39m\u001b[39m0.475\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[39m# print(f\"conf_indexes = {conf_indexes}\")\u001b[39;00m\n\u001b[1;32m     16\u001b[0m conf_pred \u001b[39m=\u001b[39m pred[conf_indexes]\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "conf_threshold = 0.5\n",
    "iou_threshold = 0.9\n",
    "\n",
    "res = []\n",
    "for idx in tqdm(range(int(len(dataset)))):\n",
    "    anno = dataset.get_ann_info(idx=idx)\n",
    "    boxes = anno[\"bboxes\"]\n",
    "    segm_path = anno[\"seg_map\"]\n",
    "    xml_path = f\"/home/aiarhipov/datasets/WTW-dataset/test/xml/{segm_path[:-4]}.xml\"\n",
    "    img_path = f\"/home/aiarhipov/datasets/WTW-dataset/test/images/{segm_path[:-4]}.jpg\"\n",
    "    \n",
    "    gt_boxes = xml_to_np(xml_path)\n",
    "    pred = inference_detector(model, img_path)[0]\n",
    "    conf_indexes = py_cpu_softnms(pred[:, :4], pred[:, 4], thresh=0.475, method=2)\n",
    "    # print(f\"conf_indexes = {conf_indexes}\")\n",
    "    conf_pred = pred[conf_indexes]\n",
    "    # print(f\"conf_pred[0] = {conf_pred[:5]}\")\n",
    "    if conf_pred.shape[0] > 1:\n",
    "        conf_pred = dbscan_result_to_aligned_result([conf_pred])\n",
    "        if conf_pred:\n",
    "            conf_pred = conf_pred[0]\n",
    "    # print(f\"conf_pred[0] = {conf_pred[:5]}\")\n",
    "    # conf_pred = pred[pred[:, 4] > conf_threshold]\n",
    "    # conf_pred = pred[:int(gt_boxes.shape[0] * 1.5)]\n",
    "    # print(\"#\"*100)\n",
    "            metrics = get_single_image_results(gt_boxes[:, :4].tolist(), conf_pred[:, :4].tolist(), iou_thr=iou_threshold)\n",
    "            \n",
    "            if (metrics[\"true_pos\"] + metrics[\"false_pos\"]) != 0:\n",
    "                metrics[\"Precision\"] = metrics[\"true_pos\"] / (metrics[\"true_pos\"] + metrics[\"false_pos\"])\n",
    "            else:\n",
    "                metrics[\"Precision\"] = 0\n",
    "                \n",
    "            if (metrics[\"true_pos\"] + metrics[\"false_neg\"]) != 0:\n",
    "                metrics[\"Recall\"] = metrics[\"true_pos\"] / (metrics[\"true_pos\"] + metrics[\"false_neg\"])\n",
    "            else:\n",
    "                metrics[\"Recall\"] = 0\n",
    "                \n",
    "            if (metrics[\"Precision\"] + metrics[\"Recall\"]) != 0:\n",
    "                metrics[\"F1\"] = 2 * metrics[\"Precision\"] * metrics[\"Recall\"] / (metrics[\"Precision\"] + metrics[\"Recall\"])\n",
    "            else:\n",
    "                metrics[\"F1\"] = 0\n",
    "                \n",
    "            res.append(metrics)\n",
    "\n",
    "Precision = np.mean([d[\"Precision\"] for d in res])\n",
    "Recall = np.mean([d[\"Recall\"] for d in res])\n",
    "F1 = np.mean([d[\"F1\"] for d in res])\n",
    "\n",
    "print(f\"Precision = {Precision}\")\n",
    "print(f\"Recall = {Recall}\")\n",
    "print(f\"F1 = {F1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUADS IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import box, Polygon\n",
    "import numpy as np\n",
    "\n",
    "# Define Each polygon \n",
    "pol1_xy = np.array([[130, 27], [129.52, 27], [129.45, 27.1], [130.13, 26]])\n",
    "pol2_xy = np.array([[30, 27.200001], [129.52, 27.34], [129.45, 27.1], [130.13, 26.950001]])\n",
    "polygon1_shape = Polygon(pol1_xy)\n",
    "polygon2_shape = Polygon(pol2_xy)\n",
    "\n",
    "# Calculate Intersection and union, and tne IOU\n",
    "polygon_intersection = polygon1_shape.intersection(polygon2_shape).area\n",
    "polygon_union = polygon1_shape.union(polygon2_shape).area\n",
    "IOU = polygon_intersection / polygon_union \n",
    "\n",
    "quad = np.array(range(9))\n",
    "quad = quad[:-1]\n",
    "quad.reshape((-1, 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUADS PHYSICAL STRUCTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=11.31s)\n",
      "creating index...\n",
      "index created!\n",
      "Using GPU=6\n",
      "load checkpoint from local path: /home/aiarhipov/centernet/exps/32_quad_long/latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-19 17:20:24,845 - root - INFO - ModulatedDeformConvPack neck.deconv_layers.0.conv is upgraded to version 2.\n",
      "2023-06-19 17:20:24,848 - root - INFO - ModulatedDeformConvPack neck.deconv_layers.2.conv is upgraded to version 2.\n",
      "2023-06-19 17:20:24,849 - root - INFO - ModulatedDeformConvPack neck.deconv_layers.4.conv is upgraded to version 2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed15bee857f4447e8659ab5a663743ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3611 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'batch_input_shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 38\u001b[0m\n\u001b[1;32m     34\u001b[0m img_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/home/aiarhipov/datasets/WTW-dataset/test/images/\u001b[39m\u001b[39m{\u001b[39;00msegm_path[:\u001b[39m-\u001b[39m\u001b[39m4\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m.jpg\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m gt_boxes \u001b[39m=\u001b[39m all_xml_to_np(xml_path)\n\u001b[0;32m---> 38\u001b[0m quads \u001b[39m=\u001b[39m detect_quads(img_path, model, integer\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     39\u001b[0m quads \u001b[39m=\u001b[39m quads[quads[:, \u001b[39m8\u001b[39m] \u001b[39m>\u001b[39m conf_threshold]\n\u001b[1;32m     41\u001b[0m boxes \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack([(quads[:, \u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m quads[:, \u001b[39m6\u001b[39m]) \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m, \n\u001b[1;32m     42\u001b[0m                    (quads[:, \u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m quads[:, \u001b[39m3\u001b[39m]) \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m, \n\u001b[1;32m     43\u001b[0m                    (quads[:, \u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m quads[:, \u001b[39m4\u001b[39m]) \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m, \n\u001b[1;32m     44\u001b[0m                    (quads[:, \u001b[39m5\u001b[39m] \u001b[39m+\u001b[39m quads[:, \u001b[39m7\u001b[39m]) \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m])\u001b[39m.\u001b[39mT\n",
      "File \u001b[0;32m~/centernet/post-processing/utils.py:147\u001b[0m, in \u001b[0;36mdetect_quads\u001b[0;34m(img_path, model, checkpoint_file, config_file, numpy, integer)\u001b[0m\n\u001b[1;32m    144\u001b[0m result_list \u001b[39m=\u001b[39m []\n\u001b[1;32m    145\u001b[0m \u001b[39mfor\u001b[39;00m img_id \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(data[\u001b[39m\"\u001b[39m\u001b[39mimg_metas\u001b[39m\u001b[39m\"\u001b[39m])):\n\u001b[1;32m    146\u001b[0m     result_list\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 147\u001b[0m         model\u001b[39m.\u001b[39;49mbbox_head\u001b[39m.\u001b[39;49m_get_bboxes_single(\n\u001b[1;32m    148\u001b[0m             center_heatmap_preds[\u001b[39m0\u001b[39;49m][img_id : img_id \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m, \u001b[39m0\u001b[39;49m:\u001b[39m1\u001b[39;49m, \u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m],\n\u001b[1;32m    149\u001b[0m             center2vertex_pred[\u001b[39m0\u001b[39;49m][img_id : img_id \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m, \u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m],\n\u001b[1;32m    150\u001b[0m             offset_preds[\u001b[39m0\u001b[39;49m][img_id : img_id \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m, \u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m],\n\u001b[1;32m    151\u001b[0m             data[\u001b[39m\"\u001b[39;49m\u001b[39mimg_metas\u001b[39;49m\u001b[39m\"\u001b[39;49m][img_id][\u001b[39m0\u001b[39;49m],\n\u001b[1;32m    152\u001b[0m             rescale\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    153\u001b[0m             with_nms\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    154\u001b[0m         )\n\u001b[1;32m    155\u001b[0m     )\n\u001b[1;32m    156\u001b[0m quads \u001b[39m=\u001b[39m result_list[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m numpy:\n",
      "File \u001b[0;32m~/mmdetection/mmdet/models/dense_heads/cycle_centernet_head.py:519\u001b[0m, in \u001b[0;36mCycleCenterNetHead._get_bboxes_single\u001b[0;34m(self, center_heatmap_pred, c2v_pred, offset_pred, img_meta, rescale, with_nms)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_bboxes_single\u001b[39m(\n\u001b[1;32m    484\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    485\u001b[0m     center_heatmap_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m     with_nms\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    491\u001b[0m ):\n\u001b[1;32m    492\u001b[0m     \u001b[39m\"\"\"Transform outputs of a single image into bbox results.\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \n\u001b[1;32m    494\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[39m            corresponding box.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    515\u001b[0m     batch_det_bboxes, batch_labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode_heatmap(\n\u001b[1;32m    516\u001b[0m         center_heatmap_pred,\n\u001b[1;32m    517\u001b[0m         c2v_pred,\n\u001b[1;32m    518\u001b[0m         offset_pred,\n\u001b[0;32m--> 519\u001b[0m         img_meta[\u001b[39m\"\u001b[39;49m\u001b[39mbatch_input_shape\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    520\u001b[0m         k\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_cfg\u001b[39m.\u001b[39mtopk,\n\u001b[1;32m    521\u001b[0m         kernel\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_cfg\u001b[39m.\u001b[39mlocal_maximum_kernel,\n\u001b[1;32m    522\u001b[0m     )\n\u001b[1;32m    523\u001b[0m     det_bboxes \u001b[39m=\u001b[39m batch_det_bboxes\u001b[39m.\u001b[39mview([\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m9\u001b[39m])\n\u001b[1;32m    524\u001b[0m     det_labels \u001b[39m=\u001b[39m batch_labels\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'batch_input_shape'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from mmcv import Config\n",
    "from mmdet.apis import set_random_seed\n",
    "from mmdet.apis import init_detector\n",
    "from mmdet.datasets import build_dataset\n",
    "from mmdet.models import build_detector\n",
    "\n",
    "from xml_to_np import xml_to_np, all_xml_to_np\n",
    "from AP import get_single_image_results\n",
    "from soft_nms import py_cpu_softnms\n",
    "from utils import detect_quads, get_free_gpu\n",
    "conf_threshold = 0.1\n",
    "iou_threshold = 0.9\n",
    "\n",
    "checkpoint_file=\"/home/aiarhipov/centernet/exps/32_quad_long/latest.pth\"\n",
    "config_file = \"/home/aiarhipov/centernet/exps/32_quad_long/config.py\"\n",
    "cfg = Config.fromfile(config_file)\n",
    "set_random_seed(0, deterministic=False)\n",
    "dataset = build_dataset(cfg.data.test)\n",
    "\n",
    "\n",
    "# model = init_detector(config_file, checkpoint_file, device=f\"cpu\")\n",
    "model = init_detector(config_file, checkpoint_file, device=f\"cuda:{get_free_gpu()}\")\n",
    "model.CLASSES = (\"box\",)\n",
    "\n",
    "res = []\n",
    "for idx in tqdm(range(int(len(dataset)))):\n",
    "    anno = dataset.get_ann_info(idx=idx)\n",
    "    boxes = anno[\"bboxes\"]\n",
    "    segm_path = anno[\"seg_map\"]\n",
    "    xml_path = f\"/home/aiarhipov/datasets/WTW-dataset/test/xml/{segm_path[:-4]}.xml\"\n",
    "    img_path = f\"/home/aiarhipov/datasets/WTW-dataset/test/images/{segm_path[:-4]}.jpg\"\n",
    "    \n",
    "    gt_boxes = all_xml_to_np(xml_path)\n",
    "\n",
    "    quads = detect_quads(img_path, model, integer=False)\n",
    "    quads = quads[quads[:, 8] > conf_threshold]\n",
    "    \n",
    "    boxes = np.vstack([(quads[:, 0] + quads[:, 6]) / 2, \n",
    "                       (quads[:, 1] + quads[:, 3]) / 2, \n",
    "                       (quads[:, 2] + quads[:, 4]) / 2, \n",
    "                       (quads[:, 5] + quads[:, 7]) / 2]).T\n",
    "    \n",
    "    nms_indexes = py_cpu_softnms(boxes, quads[:, 8], thresh=0.475, method=2)\n",
    "    nms_quads = quads[nms_indexes]\n",
    "\n",
    "    metrics = get_single_image_results(gt_boxes[:, :8], nms_quads[:, :8], iou_thr=iou_threshold, quad=True)\n",
    "    \n",
    "    if (metrics[\"true_pos\"] + metrics[\"false_pos\"]) != 0:\n",
    "        metrics[\"Precision\"] = metrics[\"true_pos\"] / (metrics[\"true_pos\"] + metrics[\"false_pos\"])\n",
    "    else:\n",
    "        metrics[\"Precision\"] = 0\n",
    "        \n",
    "    if (metrics[\"true_pos\"] + metrics[\"false_neg\"]) != 0:\n",
    "        metrics[\"Recall\"] = metrics[\"true_pos\"] / (metrics[\"true_pos\"] + metrics[\"false_neg\"])\n",
    "    else:\n",
    "        metrics[\"Recall\"] = 0\n",
    "        \n",
    "    if (metrics[\"Precision\"] + metrics[\"Recall\"]) != 0:\n",
    "        metrics[\"F1\"] = 2 * metrics[\"Precision\"] * metrics[\"Recall\"] / (metrics[\"Precision\"] + metrics[\"Recall\"])\n",
    "    else:\n",
    "        metrics[\"F1\"] = 0\n",
    "        \n",
    "    res.append(metrics)\n",
    "\n",
    "    Precision = np.mean([d[\"Precision\"] for d in res])\n",
    "    Recall = np.mean([d[\"Recall\"] for d in res])\n",
    "    F1 = np.mean([d[\"F1\"] for d in res])\n",
    "\n",
    "    print(f\"Precision = {Precision}, Recall = {Recall}, F1 = {F1}\")\n",
    "Precision = np.mean([d[\"Precision\"] for d in res])\n",
    "Recall = np.mean([d[\"Recall\"] for d in res])\n",
    "F1 = np.mean([d[\"F1\"] for d in res])\n",
    "\n",
    "print(f\"Precision = {Precision}\")\n",
    "print(f\"Recall = {Recall}\")\n",
    "print(f\"F1 = {F1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmdet_edit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "460ca83c92312ba8330185f2e6aecdc2df2cb312e6f21370f999685df633c41e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
